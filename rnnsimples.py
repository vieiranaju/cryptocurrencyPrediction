# -*- coding: utf-8 -*-
"""RNNSimples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mRpvJsxNTeG1fppjwQ4TGr1lZZEVkmS1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN
from tensorflow.keras.optimizers import Adam

# Carregar os dados CSV (certifique-se de que o caminho do arquivo está correto)
data = pd.read_csv('BTCUSDT_1D (2).csv')

# Converter a coluna 'date_time' para timestamp Unix
data['date_time'] = pd.to_datetime(data['date_time'], dayfirst=True).view(int) // 10**9

# Identificar colunas categóricas
categorical_cols = data.select_dtypes(include=['object']).columns

# Converter colunas categóricas para numéricas usando one-hot encoding
data = pd.get_dummies(data, columns=categorical_cols)

# Tratar valores ausentes imputando com a média da coluna
data.fillna(data.mean(), inplace=True)

# Separar as variáveis independentes (X) e a variável dependente (y)
X = data.drop(columns=['close'])  # Colunas independentes (exceto 'close')
y = data['close']  # Coluna 'close' como variável dependente

# Escalar os dados
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))

# Dividir os dados em conjuntos de treino (60%) e teste (40%) baseados em índices
split_index = int(len(data) * 0.6)
X_train, X_test = X_scaled[:split_index], X_scaled[split_index:]
y_train, y_test = y_scaled[:split_index], y_scaled[split_index:]

# Redimensionar os dados para o formato [amostras, timesteps, características]
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Construir o modelo RNN
model = Sequential()
model.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
model.add(Dense(1))

# Compilar o modelo
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

# Treinar o modelo
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, shuffle=False)

# Fazer previsões no conjunto de teste
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Inverter a escala dos dados
y_train_pred = scaler_y.inverse_transform(y_train_pred)
y_test_pred = scaler_y.inverse_transform(y_test_pred)
y_train = scaler_y.inverse_transform(y_train)
y_test = scaler_y.inverse_transform(y_test)

# Avaliar a performance do modelo (erro quadrático médio)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f'Train RMSE: {train_rmse:.2f}')
print(f'Test RMSE: {test_rmse:.2f}')

# Converter a coluna 'date_time' de volta para datetime para plotagem
data['date_time'] = pd.to_datetime(data['date_time'], unit='s')

# Mesclar previsões com os conjuntos de treino e teste para plotagem
train_data = pd.DataFrame(data[:split_index])
train_data['Predicted'] = y_train_pred

test_data = pd.DataFrame(data[split_index:])
test_data['Predicted'] = y_test_pred

# Ordenar os dados por date_time para plotagem adequada
train_data.sort_values(by='date_time', inplace=True)
test_data.sort_values(by='date_time', inplace=True)

# Visualizar os resultados em um gráfico
plt.figure(figsize=(14, 8))

# Plotar dados reais
plt.plot(data['date_time'], data['close'], label='Valor Real', color='green', alpha=0.6)

# Plotar dados de treino previstos
plt.plot(train_data['date_time'], train_data['Predicted'], label='Treino', color='blue', alpha=0.6)

# Plotar dados de teste previstos
plt.plot(test_data['date_time'], test_data['Predicted'], label='Teste', color='red', alpha=0.6)

plt.xlabel('Data')
plt.ylabel('Valor')
plt.title('Comparação de Valores Reais e Previstos (Treino e Teste)')
plt.legend()
plt.show()